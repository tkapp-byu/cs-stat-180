{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBBfyELRQEfJ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhodes-byu/cs-stat-180/blob/main/labs/11-gen-ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zc643pijdZd"
      },
      "source": [
        "# Lab 11: Introduction to Generative AI with Google Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H3gomFYjk8i"
      },
      "source": [
        "## Section 1\\. Overview\n",
        "\n",
        "In this lab, you will move beyond the web chat interface of Large Language Models (LLMs) and interact with Google's state-of-the-art model, **Gemini**, using Python code.\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "  * Set up the Google Generative AI SDK in a Python environment.\n",
        "  * Securely handle your API key.\n",
        "  * Send programmatic prompts to the Gemini model.\n",
        "  * Understand and control the \"temperature\" (creativity) of the model's output.\n",
        "  * Use **Few-Shot Prompting** to force the model to generate structured data (JSON) suitable for data science tasks.\n",
        "  * Learn about **Chain-of-Thought** prompting. Experiment with some simple examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA5OwYDfjrHY"
      },
      "source": [
        "## Section 2\\. Prerequisites: Getting your API Key\n",
        "\n",
        "To use the Gemini model through code, you need a unique API key. This key acts like a password that connects your script to Google's servers.\n",
        "\n",
        "1.  Go to **Google AI Studio**: `https://aistudio.google.com/`\n",
        "2.  Sign in with your Google account.\n",
        "3.  On the left sidebar, click on **\"Get API key\"**.\n",
        "4.  Click **\"Create API key in new project\"**.\n",
        "5.  **CRITICAL:** Copy the long string of letters and numbers that appears. This is your key. Do not share it publicly.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ADimy7Lj2wH"
      },
      "source": [
        "## Section 3\\. Lab Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhjBVO5FrFbF"
      },
      "source": [
        "\n",
        "### Step 1: Install the SDK\n",
        "\n",
        "First, we need to install the Python library that allows us to communicate with Google's AI models.\n",
        "\n",
        "*Run the following cell:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBaATZ-MrFbH"
      },
      "outputs": [],
      "source": [
        "# Install the Google Generative AI SDK\n",
        "!pip install -q -U google-generativeai\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bdQ_J_srFbH"
      },
      "source": [
        "### Step 2: Securely Setup your API Key\n",
        "\n",
        "It is a security best practice never to hard-code your API key directly into your code, especially if you might share this notebook. Google Colab provides a secure way to store such secrets.\n",
        "\n",
        "1.  On the left-hand sidebar of this Colab window, click the **Key icon** (Secrets).\n",
        "2.  Click **\"Add new secret\"**.\n",
        "3.  In the \"Name\" field, enter: `GOOGLE_API_KEY`\n",
        "4.  In the \"Value\" field, paste the long API key you copied from Google AI Studio.\n",
        "5.  Click the toggle button next to your new secret to enable notebook access.\n",
        "\n",
        "*Now, run the following cell to securely load your key:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXF31X-ZrFbH"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    # Access the key from Colab secrets\n",
        "    API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    # Configure the SDK with your key\n",
        "    genai.configure(api_key=API_KEY)\n",
        "\n",
        "    print(\"API Key configured successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please ensure you have correctly added the 'GOOGLE_API_KEY' secret in the sidebar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaVI51H-rFbI"
      },
      "source": [
        "### Step 3: Your First Generation\n",
        "\n",
        "Now we are ready to use the model. We will initialize the `gemini-2.5-flash` model, which is designed for text-based tasks, and send it a simple prompt.\n",
        "\n",
        "*Run the following cell:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbUndHnTrFbI"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "# Use the model name \"gemini-2.5-flash\" as a common compatible model name\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "# Define a simple prompt\n",
        "prompt = \"Explain the concept of 'Data Science' to a 10-year-old in three sentences.\"\n",
        "\n",
        "print(f\"Sending prompt: '{prompt}'...\\n\")\n",
        "\n",
        "# Generate content\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Print the result\n",
        "print(\"--- Gemini Response ---\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTwwaVi8sAG9"
      },
      "source": [
        "Analysis: How well did the LLM answer the question? How would you answer it differently? Create another prompt for a university student. How well did it answer? Try a few additional prompts until you get an answer that best fits with your understanding from the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Zw7Cc7rFbI"
      },
      "source": [
        "### Step 4: Controlling Creativity (Temperature)\n",
        "\n",
        "LLMs don't just pick the single most likely next word; they sample from a distribution of probable words. The **Temperature** parameter controls how \"risky\" this sampling is.\n",
        "\n",
        "  * **Low Temperature (e.g., 0.0 - 0.3):** The model is more deterministic, choosing the most probable words. The output is more consistent, factual, and logical. Better for data tasks.\n",
        "  * **High Temperature (e.g., 0.7 - 1.0):** The model takes more risks, choosing less probable words. The output is more creative and varied but can be less factual. Better for creative writing.\n",
        "\n",
        "Let's see this in action by asking for something creative.\n",
        "\n",
        "*Run the following cell multiple times to see how the high-temperature output changes.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VlBVuE3kl2s"
      },
      "outputs": [],
      "source": [
        "# import google.generativeai as genai  # already imported above\n",
        "\n",
        "# --- Low Temperature Run ---\n",
        "# Create a configuration for low temperature (consistent/deterministic)\n",
        "low_temp_config = genai.types.GenerationConfig(temperature=0.1)\n",
        "\n",
        "print(\"Generating with LOW temperature (0.1)...\")\n",
        "response_low = model.generate_content(\n",
        "    \"Generate a creative name for an AI start-up formed by BYU CS 180 students.\",\n",
        "    generation_config=low_temp_config\n",
        ")\n",
        "print(f\"--- Low Temp Result ---\\n{response_low.text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MsU3H1neFP"
      },
      "source": [
        "The output generates a lot of responses. How would you limit the number of responses?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9SZEUOEk2T4"
      },
      "source": [
        "### Rerun the prompt by implementing some of the suggestions in the previous output. Did this improve the output?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWA8fVGNrFbI"
      },
      "outputs": [],
      "source": [
        "# --- High Temperature Run ---\n",
        "# Create a configuration for high temperature (creative/varied)\n",
        "# Gemini's effective temperature range is typically 0.0 to 1.0\n",
        "high_temp_config = genai.types.GenerationConfig(temperature=0.9)\n",
        "\n",
        "print(\"Generating with HIGH temperature (1.0)...\")\n",
        "response_high = model.generate_content(\n",
        "    \"Generate a creative name for an AI start-up formed by BYU CS 180 students.\",\n",
        "    generation_config=high_temp_config\n",
        ")\n",
        "print(f\"--- High Temp Result ---\\n{response_high.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42FdQxys2SP"
      },
      "source": [
        "How did changing the temperature affect the responses? Try higher temperatures. What happened?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK5Cwwj2rFbI"
      },
      "source": [
        "### Step 5: Few-Shot Prompting for Structured Data\n",
        "\n",
        "This is one of the most practical techniques for a data scientist. Often, you don't want a chatty paragraph from the model; you want structured data (like JSON) that you can immediately use in a pipeline or load into a database.\n",
        "\n",
        "Modern models are good at following instructions, but they are **much better** when you provide examples of the exact input-to-output pattern you desire. This is called **Few-Shot Prompting**.\n",
        "\n",
        "In this example, we want to extract specific details from unstructured restaurant reviews and format them into strict JSON.\n",
        "\n",
        "*Run the following cell:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJYyXGiaumXv"
      },
      "outputs": [],
      "source": [
        "# Try a prompt that has no examples of the desired pattern\n",
        "# This is called a \"zero-shot\" prompt\n",
        "\n",
        "zero_shot_prompt = \"\"\"\n",
        "Your task is to analyze restaurant reviews and extract information into strict JSON format.\n",
        "Do not provide any introductory or concluding text outside the JSON block.\n",
        "\n",
        "Input Text: \"I ordered the blackened salmon. It was cooked perfectly and very flavorful, but the service was slow.\"\n",
        "Output JSON:\n",
        "\"\"\"\n",
        "# We use a low temperature because we want strict adherence to the JSON format,\n",
        "# not creative improvisation.\n",
        "formatting_config = genai.types.GenerationConfig(temperature=0.0)\n",
        "\n",
        "print(\"Sending zero-shot prompt to extract JSON...\")\n",
        "response = model.generate_content(\n",
        "    zero_shot_prompt,\n",
        "    generation_config=formatting_config\n",
        ")\n",
        "\n",
        "print(\"\\n--- Model Output ---\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNFirghhvzv-"
      },
      "source": [
        "How well did it do? What happens when you don't use examples? That is, just ask for whether the review was positive, neutral, or negative?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvBrUIJ5rFbI"
      },
      "outputs": [],
      "source": [
        "# We define a prompt that includes examples of the desired pattern.\n",
        "# This \"teaches\" the model the format we expect.\n",
        "\n",
        "few_shot_prompt = \"\"\"\n",
        "Your task is to analyze restaurant reviews and extract information into strict JSON format.\n",
        "Do not provide any introductory or concluding text outside the JSON block.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Input Text: \"The pepperoni pizza was greasy but tasty.\"\n",
        "Output JSON: {\"food_item\": \"pepperoni pizza\", \"sentiment\": \"mixed\"}\n",
        "\n",
        "Input Text: \"Worst mushroom slice I've ever had. Cold and rubbery.\"\n",
        "Output JSON: {\"food_item\": \"mushroom slice\", \"sentiment\": \"negative\"}\n",
        "\n",
        "Input Text: \"OMG the pineapple and jalapeño combo is life-changing! So good.\"\n",
        "Output JSON: {\"food_item\": \"pineapple and jalapeño combo\", \"sentiment\": \"positive\"}\n",
        "\n",
        "***\n",
        "\n",
        "Now, process this new input:\n",
        "\n",
        "Input Text: \"I ordered the blackened salmon. It was cooked perfectly and very flavorful, but the service was slow.\"\n",
        "Output JSON:\n",
        "\"\"\"\n",
        "\n",
        "# We use a low temperature because we want strict adherence to the JSON format,\n",
        "# not creative improvisation.\n",
        "formatting_config = genai.types.GenerationConfig(temperature=0.0)\n",
        "\n",
        "print(\"Sending few-shot prompt to extract JSON...\")\n",
        "response = model.generate_content(\n",
        "    few_shot_prompt,\n",
        "    generation_config=formatting_config\n",
        ")\n",
        "\n",
        "print(\"\\n--- Model Output ---\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKpgWw70taYT"
      },
      "source": [
        "How well did it do? What happens when you use few-shot prompting? How is the output different? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61201375"
      },
      "source": [
        "### Step 6: Chain-of-Thought Prompting\n",
        "\n",
        "Chain-of-Thought (CoT) prompting is a technique that enables complex reasoning capabilities in large language models. By providing intermediate reasoning steps, the model can generate more accurate results on multi-step problems.\n",
        "\n",
        "In this experiment, we'll give the model a riddle and explicitly ask it to \"think step by step\" to solve it. This encourages the model to show its reasoning process before arriving at the final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0145308"
      },
      "outputs": [],
      "source": [
        "# import google.generativeai as genai. # already imported above\n",
        "\n",
        "# Re-initialize the model if needed, or use the one from Step 3 if it's still available.\n",
        "# Use 'gemini-2.5-flash' as it was used successfully earlier in the notebook.\n",
        "# If you encountered quota errors earlier, you might need to wait or use a different model/API key.\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "chain_of_thought_prompt = \"\"\"\n",
        "A man is looking at a photograph and says, \"Brothers and sisters I have none, but that man's father is my father's son.\" Who is the man in the photograph?\n",
        "\n",
        "Let's think step by step to solve this riddle.\n",
        "\"\"\"\n",
        "\n",
        "# We'll use a low temperature to encourage focused and logical reasoning.\n",
        "cot_config = genai.types.GenerationConfig(temperature=0.2)\n",
        "\n",
        "print(\"Sending chain-of-thought prompt...\")\n",
        "response = model.generate_content(\n",
        "    chain_of_thought_prompt,\n",
        "    generation_config=cot_config\n",
        ")\n",
        "\n",
        "print(\"\\n--- Model Output (Chain-of-Thought) ---\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGEeXTOw126B"
      },
      "source": [
        "Come up with an interesting chain-of-thought problem and try it with the LLM. How well did it do? Can you figure out a problem to fool the LLM? Ask another LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B-DWTx3r9UD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcwq6qn4rFbJ"
      },
      "source": [
        "-----\n",
        "\n",
        "## Step 7\\. Submission Task\n",
        "\n",
        "**To Submit:** Upload the code file with outputs and .pdf to Canvas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J3nYyZcuKK2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
